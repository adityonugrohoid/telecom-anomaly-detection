{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network Anomaly Detection\n",
    "\n",
    "This notebook demonstrates unsupervised anomaly detection on telecom network KPI data\n",
    "using Isolation Forest. We identify anomalous cell behavior from time-series metrics\n",
    "and evaluate detection quality against ground-truth labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_context(\"notebook\")\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['figure.dpi'] = 100"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../src\")\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "DATA_PATH = \"../data/synthetic_data.parquet\"\n",
    "\n",
    "np.random.seed(RANDOM_STATE)\n",
    "print(\"Environment ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. Data Loading & Validation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(DATA_PATH)\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anomaly rate in ground-truth labels\n",
    "anomaly_col = [c for c in df.columns if 'anomaly' in c.lower() or 'label' in c.lower()][0]\n",
    "print(f\"Label column: {anomaly_col}\")\n",
    "print(f\"\\nAnomaly rate:\")\n",
    "print(df[anomaly_col].value_counts(normalize=True).round(4))\n",
    "print(f\"\\nAnomaly count: {df[anomaly_col].sum()} / {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell tower counts\n",
    "cell_col = [c for c in df.columns if 'cell' in c.lower()][0]\n",
    "print(f\"Cell column: {cell_col}\")\n",
    "print(f\"Number of unique cells: {df[cell_col].nunique()}\")\n",
    "print(f\"\\nRecords per cell (top 10):\")\n",
    "print(df[cell_col].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic validation\n",
    "print(\"Missing values:\")\n",
    "print(df.isnull().sum()[df.isnull().sum() > 0])\n",
    "if df.isnull().sum().sum() == 0:\n",
    "    print(\"No missing values found.\")\n",
    "\n",
    "print(f\"\\nData types:\\n{df.dtypes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-series plot for a sample cell\n",
    "time_col = [c for c in df.columns if 'time' in c.lower() or 'date' in c.lower() or 'timestamp' in c.lower()]\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "kpi_cols = [c for c in numeric_cols if c != anomaly_col]\n",
    "\n",
    "sample_cell = df[cell_col].value_counts().index[0]\n",
    "cell_data = df[df[cell_col] == sample_cell].copy()\n",
    "\n",
    "if time_col:\n",
    "    cell_data = cell_data.sort_values(time_col[0])\n",
    "\n",
    "fig, axes = plt.subplots(min(3, len(kpi_cols)), 1, figsize=(14, 10), sharex=True)\n",
    "if min(3, len(kpi_cols)) == 1:\n",
    "    axes = [axes]\n",
    "for i, col in enumerate(kpi_cols[:3]):\n",
    "    ax = axes[i]\n",
    "    ax.plot(cell_data.index, cell_data[col], linewidth=0.8, label=col)\n",
    "    anomaly_mask = cell_data[anomaly_col] == 1\n",
    "    ax.scatter(cell_data.index[anomaly_mask], cell_data.loc[anomaly_mask, col],\n",
    "               color='red', s=20, zorder=5, label='Anomaly')\n",
    "    ax.set_ylabel(col)\n",
    "    ax.legend(loc='upper right')\n",
    "ax.set_xlabel('Index')\n",
    "fig.suptitle(f'Time-Series KPIs for Cell: {sample_cell}', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anomaly distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "df[anomaly_col].value_counts().plot(kind='bar', ax=axes[0], color=['steelblue', 'crimson'])\n",
    "axes[0].set_title('Anomaly Label Distribution')\n",
    "axes[0].set_xlabel('Label')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_xticklabels(['Normal', 'Anomaly'], rotation=0)\n",
    "\n",
    "# Anomaly rate per cell\n",
    "cell_anomaly_rate = df.groupby(cell_col)[anomaly_col].mean().sort_values(ascending=False)\n",
    "axes[1].bar(range(len(cell_anomaly_rate)), cell_anomaly_rate.values, color='coral')\n",
    "axes[1].set_title('Anomaly Rate per Cell')\n",
    "axes[1].set_xlabel('Cell Index')\n",
    "axes[1].set_ylabel('Anomaly Rate')\n",
    "axes[1].axhline(y=cell_anomaly_rate.mean(), color='black', linestyle='--', label='Mean')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KPI distributions by label (normal vs anomaly)\n",
    "plot_cols = kpi_cols[:6]\n",
    "n_plots = len(plot_cols)\n",
    "n_rows = (n_plots + 2) // 3\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, 3, figsize=(16, 4 * n_rows))\n",
    "axes = axes.flatten() if n_rows > 1 else [axes] if n_plots == 1 else axes\n",
    "\n",
    "for i, col in enumerate(plot_cols):\n",
    "    for label, color in zip([0, 1], ['steelblue', 'crimson']):\n",
    "        subset = df[df[anomaly_col] == label][col]\n",
    "        axes[i].hist(subset, bins=50, alpha=0.6, color=color,\n",
    "                     label='Anomaly' if label == 1 else 'Normal', density=True)\n",
    "    axes[i].set_title(f'{col}')\n",
    "    axes[i].legend()\n",
    "\n",
    "for j in range(i + 1, len(axes)):\n",
    "    axes[j].set_visible(False)\n",
    "\n",
    "fig.suptitle('KPI Distributions by Label', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engineer import FeatureEngineer\n",
    "\n",
    "fe = FeatureEngineer()\n",
    "df_features = fe.fit_transform(df)\n",
    "print(f\"Feature matrix shape: {df_features.shape}\")\n",
    "print(f\"Original columns: {len(df.columns)} -> Engineered columns: {len(df_features.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect new features\n",
    "new_cols = [c for c in df_features.columns if c not in df.columns]\n",
    "print(f\"New engineered features ({len(new_cols)}):\")\n",
    "for col in new_cols:\n",
    "    print(f\"  - {col}\")\n",
    "\n",
    "df_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare feature matrix (exclude label and non-numeric columns)\n",
    "exclude_cols = [anomaly_col, cell_col] + time_col\n",
    "feature_cols = [c for c in df_features.select_dtypes(include=[np.number]).columns if c not in exclude_cols]\n",
    "X = df_features[feature_cols].copy()\n",
    "y_true = df_features[anomaly_col].values\n",
    "\n",
    "print(f\"Feature matrix X: {X.shape}\")\n",
    "print(f\"Ground truth labels y: {y_true.shape}\")\n",
    "print(f\"Features used: {feature_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import IsolationForestModel\n",
    "\n",
    "model = IsolationForestModel(random_state=RANDOM_STATE)\n",
    "print(\"Isolation Forest model initialized.\")\n",
    "print(f\"Model parameters: {model}\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train unsupervised - no labels provided\n",
    "model.fit(X)\n",
    "print(\"Model training complete (unsupervised - no labels used).\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict anomaly scores\n",
    "anomaly_scores = model.decision_function(X)\n",
    "anomaly_preds = model.predict(X)\n",
    "\n",
    "# Isolation Forest returns -1 for anomalies and 1 for normal; convert to 0/1\n",
    "anomaly_preds_binary = (anomaly_preds == -1).astype(int)\n",
    "\n",
    "print(f\"Anomaly scores range: [{anomaly_scores.min():.4f}, {anomaly_scores.max():.4f}]\")\n",
    "print(f\"Predicted anomalies: {anomaly_preds_binary.sum()} / {len(anomaly_preds_binary)}\")\n",
    "print(f\"Predicted anomaly rate: {anomaly_preds_binary.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 6. Evaluation & Metrics"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    classification_report, f1_score, precision_score,\n",
    "    recall_score, roc_auc_score, roc_curve, confusion_matrix\n",
    ")\n",
    "\n",
    "# Evaluate against ground truth\n",
    "print(\"Classification Report (Isolation Forest vs Ground Truth):\")\n",
    "print(\"=\" * 60)\n",
    "print(classification_report(y_true, anomaly_preds_binary, target_names=['Normal', 'Anomaly']))\n",
    "\n",
    "f1 = f1_score(y_true, anomaly_preds_binary)\n",
    "precision = precision_score(y_true, anomaly_preds_binary)\n",
    "recall = recall_score(y_true, anomaly_preds_binary)\n",
    "print(f\"F1 Score:   {f1:.4f}\")\n",
    "print(f\"Precision:  {precision:.4f}\")\n",
    "print(f\"Recall:     {recall:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anomaly score distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Distribution of anomaly scores by true label\n",
    "for label, color, name in zip([0, 1], ['steelblue', 'crimson'], ['Normal', 'Anomaly']):\n",
    "    mask = y_true == label\n",
    "    axes[0].hist(anomaly_scores[mask], bins=60, alpha=0.6, color=color, label=name, density=True)\n",
    "axes[0].set_title('Anomaly Score Distribution by True Label')\n",
    "axes[0].set_xlabel('Anomaly Score (lower = more anomalous)')\n",
    "axes[0].set_ylabel('Density')\n",
    "axes[0].legend()\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_true, anomaly_preds_binary)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1],\n",
    "            xticklabels=['Normal', 'Anomaly'], yticklabels=['Normal', 'Anomaly'])\n",
    "axes[1].set_title('Confusion Matrix')\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('Actual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve using anomaly scores\n",
    "# Negate scores because lower score = more anomalous in sklearn Isolation Forest\n",
    "roc_scores = -anomaly_scores\n",
    "fpr, tpr, thresholds = roc_curve(y_true, roc_scores)\n",
    "auc_score = roc_auc_score(y_true, roc_scores)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.plot(fpr, tpr, color='darkorange', lw=2, label=f'Isolation Forest (AUC = {auc_score:.4f})')\n",
    "ax.plot([0, 1], [0, 1], color='gray', lw=1, linestyle='--', label='Random')\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_title('ROC Curve - Anomaly Detection')\n",
    "ax.legend(loc='lower right')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(f\"ROC AUC Score: {auc_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 7. Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Permutation importance using the anomaly detector's decision_function\n",
    "perm_result = permutation_importance(\n",
    "    model, X, y_true,\n",
    "    n_repeats=10,\n",
    "    random_state=RANDOM_STATE,\n",
    "    scoring='f1'\n",
    ")\n",
    "\n",
    "perm_df = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance_mean': perm_result.importances_mean,\n",
    "    'importance_std': perm_result.importances_std\n",
    "}).sort_values('importance_mean', ascending=False)\n",
    "\n",
    "print(\"Feature Importance (Permutation):\")\n",
    "print(perm_df.head(15).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importance\n",
    "top_n = min(15, len(perm_df))\n",
    "top_features = perm_df.head(top_n)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "ax.barh(range(top_n), top_features['importance_mean'].values,\n",
    "        xerr=top_features['importance_std'].values,\n",
    "        color='steelblue', alpha=0.8)\n",
    "ax.set_yticks(range(top_n))\n",
    "ax.set_yticklabels(top_features['feature'].values)\n",
    "ax.invert_yaxis()\n",
    "ax.set_xlabel('Mean Importance (Permutation)')\n",
    "ax.set_title('Top Feature Importances for Anomaly Detection')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anomaly score visualization across cells\n",
    "df_features['anomaly_score'] = anomaly_scores\n",
    "df_features['predicted_anomaly'] = anomaly_preds_binary\n",
    "\n",
    "cell_scores = df_features.groupby(cell_col).agg(\n",
    "    mean_score=('anomaly_score', 'mean'),\n",
    "    min_score=('anomaly_score', 'min'),\n",
    "    anomaly_rate=('predicted_anomaly', 'mean'),\n",
    "    true_anomaly_rate=(anomaly_col, 'mean')\n",
    ").sort_values('mean_score')\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Mean anomaly score per cell\n",
    "colors = ['crimson' if rate > 0.1 else 'steelblue' for rate in cell_scores['true_anomaly_rate']]\n",
    "axes[0].bar(range(len(cell_scores)), cell_scores['mean_score'], color=colors, alpha=0.8)\n",
    "axes[0].set_title('Mean Anomaly Score per Cell')\n",
    "axes[0].set_xlabel('Cell (sorted by score)')\n",
    "axes[0].set_ylabel('Mean Anomaly Score')\n",
    "\n",
    "# Predicted vs true anomaly rate per cell\n",
    "axes[1].scatter(cell_scores['true_anomaly_rate'], cell_scores['anomaly_rate'],\n",
    "                alpha=0.7, s=60, color='darkorange')\n",
    "axes[1].plot([0, cell_scores['true_anomaly_rate'].max()],\n",
    "             [0, cell_scores['true_anomaly_rate'].max()],\n",
    "             'k--', alpha=0.5, label='Perfect')\n",
    "axes[1].set_xlabel('True Anomaly Rate')\n",
    "axes[1].set_ylabel('Predicted Anomaly Rate')\n",
    "axes[1].set_title('Predicted vs True Anomaly Rate per Cell')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 8. Business Insights & Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold sensitivity analysis for early warning\n",
    "thresholds_to_test = np.percentile(anomaly_scores, [1, 2, 5, 10, 15, 20])\n",
    "\n",
    "print(\"Threshold Tuning Analysis:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Percentile':>12} {'Threshold':>12} {'Precision':>12} {'Recall':>12} {'F1':>12}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "results = []\n",
    "for pct, thr in zip([1, 2, 5, 10, 15, 20], thresholds_to_test):\n",
    "    preds = (anomaly_scores < thr).astype(int)\n",
    "    if preds.sum() > 0:\n",
    "        p = precision_score(y_true, preds, zero_division=0)\n",
    "        r = recall_score(y_true, preds, zero_division=0)\n",
    "        f = f1_score(y_true, preds, zero_division=0)\n",
    "    else:\n",
    "        p, r, f = 0, 0, 0\n",
    "    results.append({'percentile': pct, 'threshold': thr, 'precision': p, 'recall': r, 'f1': f})\n",
    "    print(f\"{pct:>12}th {thr:>12.4f} {p:>12.4f} {r:>12.4f} {f:>12.4f}\")\n",
    "\n",
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize threshold trade-offs\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(results_df['percentile'], results_df['precision'], 'o-', label='Precision', linewidth=2)\n",
    "ax.plot(results_df['percentile'], results_df['recall'], 's-', label='Recall', linewidth=2)\n",
    "ax.plot(results_df['percentile'], results_df['f1'], 'D-', label='F1 Score', linewidth=2)\n",
    "ax.set_xlabel('Score Percentile Threshold')\n",
    "ax.set_ylabel('Metric Value')\n",
    "ax.set_title('Anomaly Detection: Precision-Recall Trade-off by Threshold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early warning: time-to-detection analysis\n",
    "print(\"Early Warning System Insights:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\")\n",
    "print(f\"Model Performance Summary:\")\n",
    "print(f\"  - ROC AUC: {auc_score:.4f}\")\n",
    "print(f\"  - Default F1: {f1:.4f} (Precision: {precision:.4f}, Recall: {recall:.4f})\")\n",
    "print(f\"\")\n",
    "print(f\"Key Findings:\")\n",
    "print(f\"  - The Isolation Forest identifies anomalies without requiring labeled data,\")\n",
    "print(f\"    making it suitable for real-time deployment where labels are unavailable.\")\n",
    "print(f\"  - Top contributing features suggest monitoring these KPIs for early warning.\")\n",
    "print(f\"  - Cell-level analysis reveals some cells are more prone to anomalies,\")\n",
    "print(f\"    indicating potential infrastructure or coverage issues.\")\n",
    "print(f\"\")\n",
    "print(f\"Recommendations:\")\n",
    "print(f\"  1. Deploy with conservative threshold (high recall) for safety-critical alerts.\")\n",
    "print(f\"  2. Use tiered alerting: high-confidence anomalies trigger immediate action,\")\n",
    "print(f\"     borderline cases flagged for human review.\")\n",
    "print(f\"  3. Retrain periodically as network patterns evolve seasonally.\")\n",
    "print(f\"  4. Focus monitoring on top features identified by permutation importance.\")\n",
    "print(f\"  5. Investigate cells with consistently high anomaly scores for root-cause analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Summary\n",
    "\n",
    "This notebook demonstrated an unsupervised anomaly detection pipeline for telecom network data:\n",
    "\n",
    "- **Data**: Loaded and validated synthetic network KPI data with ground-truth anomaly labels.\n",
    "- **EDA**: Explored time-series behavior, anomaly distributions, and KPI patterns.\n",
    "- **Features**: Applied feature engineering to enrich the raw KPI signals.\n",
    "- **Model**: Trained an Isolation Forest in a fully unsupervised manner (no labels used during training).\n",
    "- **Evaluation**: Assessed detection quality against ground truth using F1, precision, recall, and ROC AUC.\n",
    "- **Interpretation**: Identified key features driving anomaly detection and analyzed cell-level patterns.\n",
    "- **Business Value**: Threshold tuning enables early warning systems with configurable precision-recall trade-offs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}